#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸­æ–‡æ–°é—»ç¨€ç–æ£€ç´¢ç³»ç»Ÿä¸»ç¨‹åº
Chinese News Sparse Retrieval System Main Program
"""

import sys
import os
import logging
import argparse
import time
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from src.crawler.news_spider import NewsDataCollector, DataStorage
from src.preprocessing.tokenizer import OptimizedChineseTokenizer
from src.preprocessing.analyzer import RawDataAnalyzer, DataVisualizer
from src.preprocessing.tokenizer import TokenizationAnalyzer
from src.retrieval.search_engine import ChineseNewsSearchSystem, SearchInterface
from src.evaluation.metrics import RetrievalEvaluator

def setup_logging(log_level: str = "INFO"):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # åˆ›å»ºlogsç›®å½•
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # é…ç½®æ—¥å¿—
    log_file = log_dir / f"system_{int(time.time())}.log"
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    return logger

def collect_news_data(max_articles: int = 500, use_mock: bool = True) -> list:
    """æ”¶é›†æ–°é—»æ•°æ®"""
    logger = logging.getLogger(__name__)
    logger.info(f"å¼€å§‹æ”¶é›†æ–°é—»æ•°æ®ï¼Œç›®æ ‡æ•°é‡: {max_articles}")
    
    # åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨å’Œå­˜å‚¨å™¨
    collector = NewsDataCollector()
    storage = DataStorage()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æ•°æ®
    latest_data_file = storage.get_latest_raw_data()
    
    if latest_data_file:
        print(f"å‘ç°å·²å­˜åœ¨çš„æ•°æ®æ–‡ä»¶: {latest_data_file}")
        choice = input("æ˜¯å¦ä½¿ç”¨å·²å­˜åœ¨çš„æ•°æ®ï¼Ÿ(y/n): ").lower().strip()
        
        if choice == 'y':
            news_data = storage.load_raw_data(latest_data_file)
            logger.info(f"åŠ è½½å·²å­˜åœ¨æ•°æ®ï¼Œæ•°é‡: {len(news_data)}")
            return news_data
    
    # æ”¶é›†æ–°æ•°æ®
    news_data = collector.collect_news(max_articles=max_articles, use_mock=use_mock)
    
    # ä¿å­˜æ•°æ®
    saved_path = storage.save_raw_data(news_data)
    logger.info(f"æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {saved_path}")
    
    return news_data

def analyze_data(news_data: list, save_visualizations: bool = True) -> tuple:
    """åˆ†ææ•°æ®"""
    logger = logging.getLogger(__name__)
    logger.info("å¼€å§‹æ•°æ®åˆ†æ...")
    
    # 1. åŸå§‹æ•°æ®åˆ†æ
    print("\nğŸ“Š æ‰§è¡ŒåŸå§‹æ•°æ®åˆ†æ...")
    data_analyzer = RawDataAnalyzer(news_data)
    data_analysis = data_analyzer.comprehensive_analysis()
    
    # 2. åˆ†è¯åˆ†æ
    print("\nâœ‚ï¸  æ‰§è¡Œåˆ†è¯åˆ†æ...")
    tokenizer = OptimizedChineseTokenizer(
        custom_dict_path="config/news_dict.txt",
        stopwords_path="config/stopwords.txt"
    )
    
    tokenization_analyzer = TokenizationAnalyzer(tokenizer)
    tokenization_analysis = tokenization_analyzer.analyze_tokenization_results(news_data)
    
    # 3. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    if save_visualizations:
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        try:
            # æ•°æ®åˆ†æå¯è§†åŒ–
            data_visualizer = DataVisualizer(data_analysis)
            data_visualizer.create_all_visualizations()
            data_visualizer.save_analysis_report()
            
            logger.info("æ•°æ®å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
        except Exception as e:
            logger.warning(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
    
    return data_analysis, tokenization_analysis, tokenizer

def build_search_system(news_data: list, tokenizer) -> ChineseNewsSearchSystem:
    """æ„å»ºæ£€ç´¢ç³»ç»Ÿ"""
    logger = logging.getLogger(__name__)
    logger.info("å¼€å§‹æ„å»ºæ£€ç´¢ç³»ç»Ÿ...")
    
    print("\nğŸ”§ æ„å»ºæ£€ç´¢ç³»ç»Ÿ...")
    
    # åˆ›å»ºæœç´¢ç³»ç»Ÿ
    search_system = ChineseNewsSearchSystem(
        custom_dict_path="config/news_dict.txt",
        stopwords_path="config/stopwords.txt"
    )
    
    # æ„å»ºç´¢å¼•
    result = search_system.index_documents(news_data)
    
    if result["success"]:
        print(f"âœ… ç´¢å¼•æ„å»ºæˆåŠŸ!")
        print(f"   - æ–‡æ¡£æ•°é‡: {result['statistics']['æ–‡æ¡£æ•°é‡']}")
        print(f"   - æ„å»ºæ—¶é—´: {result['statistics']['æ„å»ºæ—¶é—´']}")
        print(f"   - è¯æ±‡è¡¨å¤§å°: {result['statistics']['è¯æ±‡è¡¨å¤§å°']}")
        print(f"   - å†…å­˜ä½¿ç”¨: {result['statistics']['å†…å­˜ä½¿ç”¨']}")
    else:
        print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {result['message']}")
        return None
    
    return search_system

def evaluate_system(search_system: ChineseNewsSearchSystem) -> dict:
    """è¯„ä¼°ç³»ç»Ÿæ€§èƒ½"""
    logger = logging.getLogger(__name__)
    logger.info("å¼€å§‹ç³»ç»Ÿè¯„ä¼°...")
    
    print("\nğŸ“‹ æ‰§è¡Œç³»ç»Ÿè¯„ä¼°...")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RetrievalEvaluator(search_system)
    
    # æ‰§è¡Œè¯„ä¼°
    print("   - æ£€ç´¢è´¨é‡è¯„ä¼°...")
    quality_results = evaluator.evaluate_retrieval_quality()
    
    print("   - ç³»ç»Ÿæ€§èƒ½è¯„ä¼°...")
    performance_results = evaluator.evaluate_system_performance(iterations=20)
    
    print("   - é…ç½®åŸºå‡†æµ‹è¯•...")
    benchmark_results = evaluator.benchmark_different_configurations()
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    print("   - ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    report_path = evaluator.generate_evaluation_report()
    
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    return {
        "quality": quality_results,
        "performance": performance_results,
        "benchmark": benchmark_results,
        "report_path": report_path
    }

def run_interactive_demo(search_system: ChineseNewsSearchSystem):
    """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
    print("\nğŸš€ å¯åŠ¨äº¤äº’å¼æ£€ç´¢æ¼”ç¤º...")
    
    # åˆ›å»ºäº¤äº’æ¥å£
    interface = SearchInterface(search_system)
    
    # è¿è¡Œäº¤äº’å¼æœç´¢
    interface.interactive_search()

def run_batch_test(search_system: ChineseNewsSearchSystem):
    """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
    print("\nğŸ§ª è¿è¡Œæ‰¹é‡æµ‹è¯•...")
    
    test_queries = [
        "äººå·¥æ™ºèƒ½",
        "æ–°å† ç–«æƒ…é˜²æ§", 
        "ç¢³è¾¾å³°ç¢³ä¸­å’Œ",
        "ç»æµå‘å±•æ”¿ç­–",
        "æœºå™¨å­¦ä¹ ç®—æ³•",
        "ç–«è‹—æ¥ç§",
        "ç»¿è‰²å‘å±•",
        "æ•°å­—ç»æµ",
        "æ™ºèƒ½åˆ¶é€ ",
        "5Gç½‘ç»œæŠ€æœ¯"
    ]
    
    print(f"æµ‹è¯•æŸ¥è¯¢æ•°é‡: {len(test_queries)}")
    print("-" * 50)
    
    total_time = 0
    total_results = 0
    
    for i, query in enumerate(test_queries, 1):
        print(f"{i:2d}. æŸ¥è¯¢: '{query}'")
        
        try:
            results, search_time = search_system.search(query, top_k=3)
            total_time += search_time
            total_results += len(results)
            
            print(f"    æ£€ç´¢æ—¶é—´: {search_time:.4f}ç§’, ç»“æœæ•°: {len(results)}")
            
            if results:
                for j, result in enumerate(results, 1):
                    print(f"    {j}. {result['title'][:50]}... (ç›¸ä¼¼åº¦: {result['similarity_score']:.4f})")
            else:
                print("    æ— ç›¸å…³ç»“æœ")
                
        except Exception as e:
            print(f"    âŒ æ£€ç´¢å¤±è´¥: {e}")
        
        print()
    
    print("=" * 50)
    print(f"æ‰¹é‡æµ‹è¯•å®Œæˆ:")
    print(f"  - æ€»æŸ¥è¯¢æ•°: {len(test_queries)}")
    print(f"  - æ€»è€—æ—¶: {total_time:.4f} ç§’")
    print(f"  - å¹³å‡å“åº”æ—¶é—´: {total_time/len(test_queries):.4f} ç§’")
    print(f"  - å¹³å‡ç»“æœæ•°: {total_results/len(test_queries):.1f}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸­æ–‡æ–°é—»ç¨€ç–æ£€ç´¢ç³»ç»Ÿ")
    parser.add_argument("--mode", choices=["full", "demo", "test", "collect", "analyze"], 
                       default="full", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--articles", type=int, default=500, help="çˆ¬å–æ–‡ç« æ•°é‡")
    parser.add_argument("--mock", action="store_true", help="ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    parser.add_argument("--no-viz", action="store_true", help="ä¸ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    parser.add_argument("--log-level", default="INFO", help="æ—¥å¿—çº§åˆ«")
    parser.add_argument("--save-index", type=str, help="ä¿å­˜ç´¢å¼•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--load-index", type=str, help="åŠ è½½ç´¢å¼•æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.log_level)
    
    print("ğŸ” ä¸­æ–‡æ–°é—»ç¨€ç–æ£€ç´¢ç³»ç»Ÿ")
    print("=" * 50)
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"å¯åŠ¨æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    try:
        if args.mode == "collect":
            # ä»…æ•°æ®æ”¶é›†æ¨¡å¼
            news_data = collect_news_data(args.articles, args.mock)
            print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {len(news_data)} ç¯‡æ–‡ç« ")
            
        elif args.mode == "analyze":
            # ä»…æ•°æ®åˆ†ææ¨¡å¼
            storage = DataStorage()
            latest_data_file = storage.get_latest_raw_data()
            
            if not latest_data_file:
                print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®æ”¶é›†")
                return
            
            news_data = storage.load_raw_data(latest_data_file)
            data_analysis, tokenization_analysis, tokenizer = analyze_data(news_data, not args.no_viz)
            print("âœ… æ•°æ®åˆ†æå®Œæˆ")
            
        else:
            # å®Œæ•´æµç¨‹æˆ–æ¼”ç¤ºæ¨¡å¼
            
            # 1. æ•°æ®æ”¶é›†
            if args.load_index:
                # å¦‚æœè¦åŠ è½½ç´¢å¼•ï¼Œå…ˆå°è¯•ä»ç´¢å¼•æ–‡ä»¶è·å–æ•°æ®
                print(f"ğŸ”„ åŠ è½½ç´¢å¼•æ–‡ä»¶: {args.load_index}")
                search_system = ChineseNewsSearchSystem(
                    custom_dict_path="config/news_dict.txt",
                    stopwords_path="config/stopwords.txt"
                )
                
                if search_system.load_index(args.load_index):
                    print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
                    news_data = search_system.documents
                else:
                    print("âŒ ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œè½¬ä¸ºé‡æ–°æ„å»º")
                    news_data = collect_news_data(args.articles, args.mock)
                    search_system = None
            else:
                news_data = collect_news_data(args.articles, args.mock)
                search_system = None
            
            # 2. æ•°æ®åˆ†æ
            if args.mode == "full":
                data_analysis, tokenization_analysis, tokenizer = analyze_data(news_data, not args.no_viz)
            
            # 3. æ„å»ºæœç´¢ç³»ç»Ÿ
            if search_system is None:
                search_system = build_search_system(news_data, tokenizer if 'tokenizer' in locals() else None)
            
            if search_system is None:
                print("âŒ æœç´¢ç³»ç»Ÿæ„å»ºå¤±è´¥")
                return
            
            # 4. ä¿å­˜ç´¢å¼•
            if args.save_index:
                if search_system.save_index(args.save_index):
                    print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {args.save_index}")
                else:
                    print("âŒ ç´¢å¼•ä¿å­˜å¤±è´¥")
            
            # 5. ç³»ç»Ÿè¯„ä¼°
            if args.mode == "full":
                evaluation_results = evaluate_system(search_system)
            
            # 6. è¿è¡Œæ¼”ç¤ºæˆ–æµ‹è¯•
            if args.mode in ["demo", "full"]:
                print("\né€‰æ‹©è¿è¡Œæ¨¡å¼:")
                print("1. äº¤äº’å¼æ£€ç´¢æ¼”ç¤º")
                print("2. æ‰¹é‡æµ‹è¯•")
                print("3. è·³è¿‡æ¼”ç¤º")
                
                choice = input("è¯·é€‰æ‹© (1/2/3): ").strip()
                
                if choice == "1":
                    run_interactive_demo(search_system)
                elif choice == "2":
                    run_batch_test(search_system)
                elif choice == "3":
                    print("è·³è¿‡æ¼”ç¤º")
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè·³è¿‡æ¼”ç¤º")
            
            elif args.mode == "test":
                run_batch_test(search_system)
        
        print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ! æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ä¸­æ–‡æ–°é—»ç¨€ç–æ£€ç´¢ç³»ç»Ÿ!")

if __name__ == "__main__":
    start_time = time.time()
    main()
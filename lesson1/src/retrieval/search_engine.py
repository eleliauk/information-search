#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import time
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import logging
import json

from ..preprocessing.tokenizer import OptimizedChineseTokenizer
from ..preprocessing.analyzer import RawDataAnalyzer
from ..preprocessing.tokenizer import TokenizationAnalyzer
from .tfidf import TFIDFRetrieval
from .similarity import CosineRetrieval, SimilarityAnalyzer


class ChineseNewsSearchSystem:
    """ä¸­æ–‡æ–°é—»æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, custom_dict_path: Optional[str] = None, 
                 stopwords_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æœç´¢ç³»ç»Ÿ
        
        Args:
            custom_dict_path: è‡ªå®šä¹‰è¯å…¸è·¯å¾„
            stopwords_path: åœç”¨è¯æ–‡ä»¶è·¯å¾„
        """
        self.tokenizer = OptimizedChineseTokenizer(custom_dict_path, stopwords_path)
        self.tfidf_retrieval = TFIDFRetrieval(self.tokenizer)
        self.cosine_retrieval = None
        self.similarity_analyzer = None
        
        self.documents = []
        self.is_indexed = False
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.system_stats = {
            "index_build_time": 0,
            "avg_search_time": 0,
            "total_searches": 0,
            "last_update_time": None
        }
        
        # åˆ†æç»“æœç¼“å­˜
        self.data_analysis = None
        self.tokenization_analysis = None
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("ä¸­æ–‡æ–°é—»æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def index_documents(self, documents: List[Dict[str, Any]], save_analysis: bool = True) -> Dict[str, Any]:
        """
        å»ºç«‹æ–‡æ¡£ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            save_analysis: æ˜¯å¦ä¿å­˜åˆ†æç»“æœ
            
        Returns:
            ç´¢å¼•æ„å»ºç»“æœ
        """
        self.logger.info(f"å¼€å§‹å»ºç«‹æ–‡æ¡£ç´¢å¼•ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
        start_time = time.time()
        
        try:
            self.documents = documents
            
            # 1. åŸå§‹æ•°æ®åˆ†æ
            self.logger.info("æ‰§è¡ŒåŸå§‹æ•°æ®åˆ†æ...")
            data_analyzer = RawDataAnalyzer(documents)
            self.data_analysis = data_analyzer.comprehensive_analysis()
            
            # 2. åˆ†è¯åˆ†æ
            self.logger.info("æ‰§è¡Œåˆ†è¯åˆ†æ...")
            tokenization_analyzer = TokenizationAnalyzer(self.tokenizer)
            self.tokenization_analysis = tokenization_analyzer.analyze_tokenization_results(documents)
            
            # 3. æ„å»ºTF-IDFæ£€ç´¢æ¨¡å‹
            self.logger.info("æ„å»ºTF-IDFæ£€ç´¢æ¨¡å‹...")
            self.tfidf_retrieval.fit(documents)
            
            # 4. åˆå§‹åŒ–ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢å™¨
            self.cosine_retrieval = CosineRetrieval(self.tfidf_retrieval, documents)
            self.similarity_analyzer = SimilarityAnalyzer(self.cosine_retrieval)
            
            # 5. æ›´æ–°ç³»ç»ŸçŠ¶æ€
            build_time = time.time() - start_time
            self.system_stats["index_build_time"] = build_time
            self.system_stats["last_update_time"] = time.strftime('%Y-%m-%d %H:%M:%S')
            self.is_indexed = True
            
            # 6. ä¿å­˜åˆ†æç»“æœ
            if save_analysis:
                self._save_analysis_results()
            
            result = {
                "success": True,
                "message": "ç´¢å¼•æ„å»ºå®Œæˆ",
                "statistics": {
                    "æ–‡æ¡£æ•°é‡": len(documents),
                    "æ„å»ºæ—¶é—´": f"{build_time:.2f} ç§’",
                    "è¯æ±‡è¡¨å¤§å°": self.tfidf_retrieval.vectorizer.get_vocabulary_size(),
                    "TF-IDFçŸ©é˜µå½¢çŠ¶": self.tfidf_retrieval.doc_vectors.shape,
                    "å†…å­˜ä½¿ç”¨": f"{self.tfidf_retrieval.doc_vectors.data.nbytes / (1024 * 1024):.2f} MB"
                }
            }
            
            self.logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {build_time:.2f}ç§’")
            return result
            
        except Exception as e:
            self.logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}",
                "statistics": {}
            }
    
    def search(self, query: str, top_k: int = 10, similarity_threshold: float = 0.01,
               explain: bool = False) -> Tuple[List[Dict[str, Any]], float]:
        """
        æ‰§è¡Œæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            explain: æ˜¯å¦è¿”å›è¯¦ç»†è§£é‡Š
            
        Returns:
            æ£€ç´¢ç»“æœå’Œæ£€ç´¢æ—¶é—´
        """
        if not self.is_indexed:
            raise ValueError("ç³»ç»Ÿæœªå»ºç«‹ç´¢å¼•ï¼Œè¯·å…ˆè°ƒç”¨ index_documents()")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œæ£€ç´¢
            results = self.cosine_retrieval.search(
                query, 
                top_k=top_k, 
                similarity_threshold=similarity_threshold
            )
            
            # å¦‚æœéœ€è¦è¯¦ç»†è§£é‡Š
            if explain and results:
                for result in results:
                    doc_id = result['document_id']
                    explanation = self.cosine_retrieval.explain_similarity(query, doc_id)
                    result['explanation'] = explanation
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            search_time = time.time() - start_time
            self._update_search_stats(search_time)
            
            return results, search_time
            
        except Exception as e:
            self.logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return [], 0.0
    
    def explain_search(self, query: str, document_id: int) -> Dict[str, Any]:
        """
        è§£é‡Šæ£€ç´¢ç»“æœ
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            document_id: æ–‡æ¡£ID
            
        Returns:
            è¯¦ç»†è§£é‡Šä¿¡æ¯
        """
        if not self.is_indexed:
            raise ValueError("ç³»ç»Ÿæœªå»ºç«‹ç´¢å¼•ï¼Œè¯·å…ˆè°ƒç”¨ index_documents()")
        
        return self.cosine_retrieval.explain_similarity(query, document_id)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_indexed:
            return {"é”™è¯¯": "ç³»ç»Ÿæœªå»ºç«‹ç´¢å¼•"}
        
        # åŸºç¡€ç»Ÿè®¡
        basic_stats = {
            "ç´¢å¼•çŠ¶æ€": "å·²å»ºç«‹" if self.is_indexed else "æœªå»ºç«‹",
            "æ–‡æ¡£æ•°é‡": len(self.documents),
            "è¯æ±‡è¡¨å¤§å°": self.tfidf_retrieval.vectorizer.get_vocabulary_size(),
            "ç´¢å¼•æ„å»ºæ—¶é—´": f"{self.system_stats['index_build_time']:.2f} ç§’",
            "å¹³å‡æ£€ç´¢æ—¶é—´": f"{self.system_stats['avg_search_time']:.4f} ç§’",
            "æ€»æ£€ç´¢æ¬¡æ•°": self.system_stats["total_searches"],
            "æœ€åæ›´æ–°æ—¶é—´": self.system_stats["last_update_time"]
        }
        
        # TF-IDFæ¨¡å‹ç»Ÿè®¡
        tfidf_stats = self.tfidf_retrieval.get_model_statistics()
        
        # åˆ†è¯ç»Ÿè®¡
        tokenization_stats = self.tokenizer.get_tokenization_stats()
        
        return {
            "åŸºç¡€ç»Ÿè®¡": basic_stats,
            "TF-IDFæ¨¡å‹": tfidf_stats,
            "åˆ†è¯ç»Ÿè®¡": {
                "æ–‡æ¡£æ€»æ•°": tokenization_stats.get("total_documents", 0),
                "è¯æ±‡æ€»æ•°": tokenization_stats.get("total_tokens", 0),
                "å”¯ä¸€è¯æ±‡æ•°": tokenization_stats.get("unique_tokens_count", 0),
                "è¯æ±‡ä¸°å¯Œåº¦": tokenization_stats.get("vocabulary_richness", 0),
                "å¹³å‡æ¯æ–‡æ¡£è¯æ•°": tokenization_stats.get("avg_tokens_per_doc", 0)
            }
        }
    
    def get_data_analysis(self) -> Optional[Dict[str, Any]]:
        """è·å–æ•°æ®åˆ†æç»“æœ"""
        return self.data_analysis
    
    def get_tokenization_analysis(self) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†è¯åˆ†æç»“æœ"""
        return self.tokenization_analysis
    
    def analyze_query_performance(self, test_queries: List[str]) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢æ€§èƒ½
        
        Args:
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            æ€§èƒ½åˆ†æç»“æœ
        """
        if not self.is_indexed:
            raise ValueError("ç³»ç»Ÿæœªå»ºç«‹ç´¢å¼•ï¼Œè¯·å…ˆè°ƒç”¨ index_documents()")
        
        return self.similarity_analyzer.analyze_query_performance(test_queries)
    
    def benchmark_search_methods(self, query: str) -> Dict[str, Any]:
        """
        åŸºå‡†æµ‹è¯•ä¸åŒæœç´¢æ–¹æ³•
        
        Args:
            query: æµ‹è¯•æŸ¥è¯¢
            
        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        if not self.is_indexed:
            raise ValueError("ç³»ç»Ÿæœªå»ºç«‹ç´¢å¼•ï¼Œè¯·å…ˆè°ƒç”¨ index_documents()")
        
        return self.similarity_analyzer.compare_retrieval_methods(query)
    
    def get_similarity_distribution(self, query: str) -> Dict[str, Any]:
        """
        è·å–æŸ¥è¯¢çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†å¸ƒç»Ÿè®¡
        """
        if not self.is_indexed:
            raise ValueError("ç³»ç»Ÿæœªå»ºç«‹ç´¢å¼•ï¼Œè¯·å…ˆè°ƒç”¨ index_documents()")
        
        return self.cosine_retrieval.get_similarity_distribution(query)
    
    def save_index(self, filepath: str) -> bool:
        """
        ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if not self.is_indexed:
            self.logger.error("æ²¡æœ‰å¯ä¿å­˜çš„ç´¢å¼•")
            return False
        
        try:
            index_data = {
                'tfidf_retrieval': self.tfidf_retrieval,
                'documents': self.documents,
                'system_stats': self.system_stats,
                'data_analysis': self.data_analysis,
                'tokenization_analysis': self.tokenization_analysis,
                'is_indexed': self.is_indexed
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(index_data, f)
            
            self.logger.info(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def load_index(self, filepath: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            with open(filepath, 'rb') as f:
                index_data = pickle.load(f)
            
            self.tfidf_retrieval = index_data['tfidf_retrieval']
            self.documents = index_data['documents']
            self.system_stats = index_data['system_stats']
            self.data_analysis = index_data.get('data_analysis')
            self.tokenization_analysis = index_data.get('tokenization_analysis')
            self.is_indexed = index_data['is_indexed']
            
            # é‡æ–°åˆå§‹åŒ–æ£€ç´¢å™¨
            self.cosine_retrieval = CosineRetrieval(self.tfidf_retrieval, self.documents)
            self.similarity_analyzer = SimilarityAnalyzer(self.cosine_retrieval)
            
            self.logger.info(f"ç´¢å¼•å·²ä» {filepath} åŠ è½½")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def _update_search_stats(self, search_time: float):
        """æ›´æ–°æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        self.system_stats["total_searches"] += 1
        
        # æ›´æ–°å¹³å‡æœç´¢æ—¶é—´
        total_time = (self.system_stats["avg_search_time"] * 
                     (self.system_stats["total_searches"] - 1) + search_time)
        self.system_stats["avg_search_time"] = total_time / self.system_stats["total_searches"]
    
    def _save_analysis_results(self):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºåˆ†æç»“æœç›®å½•
            analysis_dir = Path("data/analysis")
            analysis_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ•°æ®åˆ†æç»“æœ
            if self.data_analysis:
                with open(analysis_dir / "data_analysis.json", 'w', encoding='utf-8') as f:
                    json.dump(self.data_analysis, f, ensure_ascii=False, indent=2, default=str)
            
            # ä¿å­˜åˆ†è¯åˆ†æç»“æœ
            if self.tokenization_analysis:
                with open(analysis_dir / "tokenization_analysis.json", 'w', encoding='utf-8') as f:
                    json.dump(self.tokenization_analysis, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info("åˆ†æç»“æœå·²ä¿å­˜")
            
        except Exception as e:
            self.logger.warning(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")


class SearchInterface:
    """æœç´¢ç³»ç»Ÿäº¤äº’æ¥å£"""
    
    def __init__(self, search_system: ChineseNewsSearchSystem):
        self.search_system = search_system
        self.logger = logging.getLogger(__name__)
    
    def interactive_search(self):
        """äº¤äº’å¼æ£€ç´¢ç•Œé¢"""
        print("\n" + "="*50)
        print("ğŸ” ä¸­æ–‡æ–°é—»æ£€ç´¢ç³»ç»Ÿ")
        print("="*50)
        
        if not self.search_system.is_indexed:
            print("âŒ ç³»ç»Ÿå°šæœªå»ºç«‹ç´¢å¼•ï¼Œè¯·å…ˆåŠ è½½æ•°æ®å¹¶å»ºç«‹ç´¢å¼•")
            return
        
        stats = self.search_system.get_system_stats()
        print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€:")
        print(f"   - æ–‡æ¡£æ•°é‡: {stats['åŸºç¡€ç»Ÿè®¡']['æ–‡æ¡£æ•°é‡']}")
        print(f"   - è¯æ±‡è¡¨å¤§å°: {stats['åŸºç¡€ç»Ÿè®¡']['è¯æ±‡è¡¨å¤§å°']}")
        print(f"   - å¹³å‡æ£€ç´¢æ—¶é—´: {stats['åŸºç¡€ç»Ÿè®¡']['å¹³å‡æ£€ç´¢æ—¶é—´']}")
        
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   - ç›´æ¥è¾“å…¥å…³é”®è¯è¿›è¡Œæ£€ç´¢")
        print("   - è¾“å…¥ 'stats' æŸ¥çœ‹ç³»ç»Ÿç»Ÿè®¡")
        print("   - è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        print("   - è¾“å…¥ 'quit' é€€å‡ºç³»ç»Ÿ")
        print("-"*50)
        
        while True:
            try:
                query = input("\nğŸ” è¯·è¾“å…¥æŸ¥è¯¢: ").strip()
                
                if query.lower() == 'quit':
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break
                
                elif query.lower() == 'stats':
                    self._show_system_stats()
                
                elif query.lower() == 'help':
                    self._show_help()
                
                elif not query:
                    continue
                
                else:
                    self._execute_search(query)
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    def _execute_search(self, query: str):
        """æ‰§è¡Œæœç´¢å¹¶æ˜¾ç¤ºç»“æœ"""
        print(f"\nğŸ” æ­£åœ¨æœç´¢: '{query}'...")
        
        try:
            results, search_time = self.search_system.search(query, top_k=5)
            
            print(f"â±ï¸  æ£€ç´¢æ—¶é—´: {search_time:.4f} ç§’")
            print(f"ğŸ“„ æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœ:\n")
            
            if not results:
                print("ğŸ˜” æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯")
                return
            
            for i, result in enumerate(results, 1):
                doc = result['document']
                score = result['similarity_score']
                
                print(f"ğŸ“° {i}. ç›¸ä¼¼åº¦: {score:.4f}")
                print(f"   ğŸ“Œ æ ‡é¢˜: {doc['title']}")
                print(f"   ğŸ“‚ åˆ†ç±»: {doc.get('category', 'æœªåˆ†ç±»')}")
                print(f"   ğŸ“ æ‘˜è¦: {doc['content'][:150]}...")
                print(f"   ğŸ“Š å­—æ•°: {doc.get('word_count', 0)} å­—")
                
                if i < len(results):
                    print("   " + "-"*40)
                    
        except Exception as e:
            print(f"âŒ æ£€ç´¢å‡ºé”™: {e}")
    
    def _show_system_stats(self):
        """æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
        print("-"*30)
        
        stats = self.search_system.get_system_stats()
        
        # åŸºç¡€ç»Ÿè®¡
        basic = stats.get("åŸºç¡€ç»Ÿè®¡", {})
        print("ğŸ”§ åŸºç¡€ä¿¡æ¯:")
        for key, value in basic.items():
            print(f"   - {key}: {value}")
        
        # TF-IDFç»Ÿè®¡
        tfidf = stats.get("TF-IDFæ¨¡å‹", {})
        if tfidf:
            print("\nğŸ§® TF-IDFæ¨¡å‹:")
            for key, value in list(tfidf.items())[:5]:  # åªæ˜¾ç¤ºå‰å‡ é¡¹
                print(f"   - {key}: {value}")
        
        # åˆ†è¯ç»Ÿè®¡
        token = stats.get("åˆ†è¯ç»Ÿè®¡", {})
        if token:
            print("\nâœ‚ï¸  åˆ†è¯ç»Ÿè®¡:")
            for key, value in token.items():
                print(f"   - {key}: {value}")
    
    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\nâ“ å¸®åŠ©ä¿¡æ¯:")
        print("-"*20)
        print("ğŸ” æ£€ç´¢æŠ€å·§:")
        print("   1. ä½¿ç”¨å…·ä½“çš„å…³é”®è¯ï¼Œå¦‚ï¼š'äººå·¥æ™ºèƒ½'ã€'ç–«æƒ…é˜²æ§'")
        print("   2. æ”¯æŒå¤šè¯æŸ¥è¯¢ï¼Œå¦‚ï¼š'äººå·¥æ™ºèƒ½ åŒ»ç–—åº”ç”¨'")
        print("   3. ä½¿ç”¨ä¸“ä¸šæœ¯è¯­å¯ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ")
        print("   4. é¿å…ä½¿ç”¨è¿‡äºå¸¸è§çš„è¯æ±‡")
        
        print("\nâš™ï¸  ç³»ç»Ÿå‘½ä»¤:")
        print("   - stats: æŸ¥çœ‹ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
        print("   - help:  æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print("   - quit:  é€€å‡ºç³»ç»Ÿ")
        
        print("\nğŸ’¡ ç¤ºä¾‹æŸ¥è¯¢:")
        print("   - æ–°å† ç–«æƒ…")
        print("   - äººå·¥æ™ºèƒ½æŠ€æœ¯")
        print("   - ç¢³è¾¾å³°ç¢³ä¸­å’Œ")
        print("   - ç»æµå‘å±•æ”¿ç­–")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_documents = [
        {
            "article_id": "test1",
            "title": "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
            "content": "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒåŒ…æ‹¬ç–¾ç—…è¯Šæ–­ã€è¯ç‰©ç ”å‘ã€åŒ»ç–—å½±åƒåˆ†æç­‰æ–¹é¢ã€‚æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«ç–¾ç—…ç—‡çŠ¶ï¼Œæé«˜è¯Šæ–­æ•ˆç‡ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨åŒ»ç–—å½±åƒåˆ†æä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿè¯†åˆ«Xå…‰ç‰‡ã€CTæ‰«æç­‰åŒ»ç–—å›¾åƒä¸­çš„å¼‚å¸¸æƒ…å†µã€‚",
            "category": "ç§‘æŠ€",
            "word_count": 150
        },
        {
            "article_id": "test2", 
            "title": "æ–°å† ç–«æƒ…é˜²æ§å–å¾—é‡è¦è¿›å±•",
            "content": "æ–°å† ç–«æƒ…é˜²æ§å·¥ä½œå–å¾—é‡è¦è¿›å±•ï¼Œç–«è‹—æ¥ç§ç‡æŒç»­æå‡ã€‚å…¨å›½å¤šåœ°å»ºç«‹äº†å®Œå–„çš„ç–«æƒ…é˜²æ§ä½“ç³»ï¼ŒåŒ…æ‹¬æ ¸é…¸æ£€æµ‹ã€ç–«è‹—æ¥ç§ã€éš”ç¦»ç®¡æ§ç­‰æªæ–½ã€‚å¥åº·ç ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ï¼Œä¸ºç–«æƒ…é˜²æ§æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚å„çº§æ”¿åºœç§¯æå“åº”ï¼Œç¡®ä¿äººæ°‘ç¾¤ä¼—çš„ç”Ÿå‘½å¥åº·å®‰å…¨ã€‚",
            "category": "ç¤¾ä¼š",
            "word_count": 120
        },
        {
            "article_id": "test3",
            "title": "ç¢³è¾¾å³°ç¢³ä¸­å’Œç›®æ ‡æ¨åŠ¨ç»¿è‰²å‘å±•",
            "content": "ç¢³è¾¾å³°ç¢³ä¸­å’Œç›®æ ‡çš„æå‡ºï¼Œä¸ºæˆ‘å›½ç»¿è‰²å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚æ–°èƒ½æºäº§ä¸šå¿«é€Ÿå‘å±•ï¼Œå¤ªé˜³èƒ½ã€é£èƒ½ç­‰å¯å†ç”Ÿèƒ½æºè£…æœºå®¹é‡å¤§å¹…å¢é•¿ã€‚èŠ‚èƒ½å‡æ’æŠ€æœ¯ä¸æ–­åˆ›æ–°ï¼Œç»¿è‰²åˆ¶é€ ã€æ¸…æ´ç”Ÿäº§æˆä¸ºä¼ä¸šå‘å±•çš„é‡è¦æ–¹å‘ã€‚ç”Ÿæ€ç¯å¢ƒä¿æŠ¤å·¥ä½œå–å¾—æ˜¾è‘—æˆæ•ˆã€‚",
            "category": "ç¯ä¿",
            "word_count": 110
        }
    ]
    
    print("=== æœç´¢å¼•æ“æµ‹è¯• ===")
    
    # åˆ›å»ºæœç´¢ç³»ç»Ÿ
    search_system = ChineseNewsSearchSystem()
    
    # å»ºç«‹ç´¢å¼•
    result = search_system.index_documents(test_documents)
    print(f"ç´¢å¼•æ„å»ºç»“æœ: {result}")
    
    # æ‰§è¡Œæœç´¢
    test_queries = ["äººå·¥æ™ºèƒ½", "ç–«æƒ…é˜²æ§", "ç»¿è‰²å‘å±•"]
    
    for query in test_queries:
        print(f"\n--- æœç´¢: '{query}' ---")
        results, search_time = search_system.search(query, top_k=2)
        
        print(f"æ£€ç´¢æ—¶é—´: {search_time:.4f} ç§’")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['title']} (ç›¸ä¼¼åº¦: {result['similarity_score']:.4f})")
    
    # æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡
    print(f"\n--- ç³»ç»Ÿç»Ÿè®¡ ---")
    stats = search_system.get_system_stats()
    print(f"åŸºç¡€ç»Ÿè®¡: {stats['åŸºç¡€ç»Ÿè®¡']}")
    
    print("\næœç´¢å¼•æ“æµ‹è¯•å®Œæˆï¼")